{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T22:32:18.131229Z",
     "start_time": "2021-02-04T22:32:17.441603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HDTStore at ...\n",
      "Number of RDF triples: 1137003322\n",
      "Number of subjects: 68940263\n",
      "Number of predicates: 123002\n",
      "Number of objects: 237607127\n",
      "Number of shared subject-object: 50711395\n"
     ]
    }
   ],
   "source": [
    "from rdflib_hdt import HDTStore\n",
    "from rdflib import Graph\n",
    "\n",
    "# Load an HDT file. Missing indexes are generated automatically\n",
    "# You can provide the index file by putting it in the same directory as the HDT file.\n",
    "# See https://www.rdfhdt.org/datasets/ for getting the HDT and the index file.\n",
    "print(\"Loading HDTStore at ...\")\n",
    "store = HDTStore(\"./dbpedia2016-10.hdt.1\")\n",
    "\n",
    "# Display some metadata about the HDT document itself\n",
    "print(f\"Number of RDF triples: {len(store)}\")\n",
    "print(f\"Number of subjects: {store.nb_subjects}\")\n",
    "print(f\"Number of predicates: {store.nb_predicates}\")\n",
    "print(f\"Number of objects: {store.nb_objects}\")\n",
    "print(f\"Number of shared subject-object: {store.nb_shared}\")\n",
    "\n",
    "# Create an RDFlib Graph with the HDT document as a backend\n",
    "graph = Graph(store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T22:32:21.600200Z",
     "start_time": "2021-02-04T22:32:21.426576Z"
    }
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib_hdt import HDTStore\n",
    "from rdflib import Graph, URIRef, Namespace\n",
    "from rdflib.namespace import OWL, RDF\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_wikidata_entity(entity, graph, knowledgeGraph, next_queue):\n",
    "  # Append all triples, where the object is not in English Language, not a Wikidata-Entitiy (e.g. an Identifier object) to the Knowledge Graph\n",
    "  # Set all unvisited URIRefs of objects to next_queue\n",
    "\n",
    "  for s,p,o in graph.triples((URIRef(entity), None, None)):\n",
    "      if p == OWL.sameAs:\n",
    "        print(f\"{s} has owl:sameAs and is thus possible wrong.\")\n",
    "\n",
    "      # Get Literal either if its English or a not from <class \"str\">\n",
    "      if isinstance(o, rdflib.term.Literal):\n",
    "        if isinstance(o.value, str):\n",
    "          if o.language == \"en\":\n",
    "            knowledgeGraph.add((s, p, o))\n",
    "            continue\n",
    "        else:\n",
    "          knowledgeGraph.add((s, p, o))\n",
    "          continue\n",
    "\n",
    "      # Add all URIs of the Objects to the new_queue\n",
    "      if isinstance(o, rdflib.term.URIRef):\n",
    "        if o.startswith(\"http://www.wikidata.org/entity/Q\"):\n",
    "          knowledgeGraph.add((s, p, o))\n",
    "          next_queue.add(o)\n",
    "          continue\n",
    "\n",
    "      # If so far no triple has been found, the current triple doesn't satisify the criteria\n",
    "\n",
    "  return knowledgeGraph, next_queue\n",
    "\n",
    "def process_dpbedia_entity(entity, graph, knowledgeGraph, next_queue):\n",
    "    # Append all triples to the Knowledge Graph\n",
    "    # Set all unvisited URIRefs of objects to next_queue\n",
    "\n",
    "    for s, p, o in graph.triples((URIRef(entity), None, None)):\n",
    "        \n",
    "      if p != OWL.sameAs:\n",
    "        knowledgeGraph.add((s, p, o))\n",
    "        \n",
    "        # Add all URIs of the Objects to the new_queue\n",
    "        if isinstance(o, URIRef):\n",
    "          next_queue.add(o)\n",
    "\n",
    "    return knowledgeGraph, next_queue\n",
    "\n",
    "def expand_kg(kg_type, data, graph, num_hops = 2):\n",
    "  # Run BFS to generate Knowledge Graph\n",
    "\n",
    "  assert data in [\"movielens\", \"lastfm\"]\n",
    "  assert kg_type in [\"DBpedia\", \"Wikidata\"]\n",
    "\n",
    "  print(\"_____________________\")\n",
    "  print(\"Running queue for...\")\n",
    "  print(f\"Data:\\t\\t {data:>15}\")\n",
    "  print(f\"Knowledge Graph:\\t  {kg_type:>15}\")\n",
    "  print(f\"Number of Hops:\\t  {num_hops:>15}\")\n",
    "  print(\"_____________________\")\n",
    "\n",
    "  # Get Mappings\n",
    "  data_df = pd.read_csv(f'/workspace/{data}/Mapping2{kg_type}-1.2-corrected.tsv', sep='\\t', header=None, engine='python')\n",
    "  ids = list(data_df[0])\n",
    "  names = list(data_df[1])\n",
    "  entities = list(data_df[2])\n",
    "\n",
    "  # Initialize Knowledge Graph\n",
    "  knowledgeGraph = Graph()\n",
    "\n",
    "  #Initialize a queue\n",
    "  queue = entities \n",
    "\n",
    "  # List to keep track of visited nodes.\n",
    "  visited = [] \n",
    "\n",
    "  # Run BFS\n",
    "  for i in range(0, num_hops):\n",
    "    print(f\"Parse all URls {i+1} hop(s) away...\")\n",
    "    print(f\"About to process {len(queue)} entities\")\n",
    "    next_queue = set()\n",
    "\n",
    "    # Run BFS for DBPedia\n",
    "    if kg_type == \"DBpedia\":\n",
    "      for j, entity in enumerate(tqdm(queue)):\n",
    "        if entity not in visited:\n",
    "          knowledgeGraph, next_queue = process_dpbedia_entity(entity, graph, knowledgeGraph, next_queue)\n",
    "          visited.append(entity)\n",
    "    \n",
    "    # Run BFS for Wikidata\n",
    "    else: \n",
    "      for j, entity in enumerate(tqdm(queue)):\n",
    "        if entity not in visited:\n",
    "          knowledgeGraph, next_queue = process_wikidata_entity(entity, graph, knowledgeGraph, next_queue)\n",
    "          visited.append(entity)\n",
    "    \n",
    "    print(f\"Finished {i+1} hop(s).\\n\")\n",
    "    queue = next_queue\n",
    "\n",
    "  return knowledgeGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens\n",
    "## DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T22:09:59.992734Z",
     "start_time": "2021-02-04T21:02:44.526741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/3244 [00:00<00:19, 166.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________\n",
      "Running queue for...\n",
      "Data:\t\t       movielens\n",
      "Knowledge Graph:\t          DBpedia\n",
      "Number of Hops:\t                2\n",
      "_____________________\n",
      "Parse all URls 1 hop(s) away...\n",
      "About to process 3244 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3244/3244 [00:21<00:00, 153.69it/s]\n",
      "  0%|          | 23/105964 [00:00<07:43, 228.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 hop(s).\n",
      "\n",
      "Parse all URls 2 hop(s) away...\n",
      "About to process 105964 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105964/105964 [1:06:54<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2 hop(s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = \"movielens\"\n",
    "kg_type = \"DBpedia\"\n",
    "\n",
    "movielens_dbpedia_2hops = expand_kg(kg_type, data, graph, num_hops = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T22:20:38.304005Z",
     "start_time": "2021-02-04T22:14:22.108250Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_file = \"./movielens/2hopsDBpedia.nt\"\n",
    "movielens_dbpedia_2hops.serialize(destination=graph_file, format='nt')\n",
    "\n",
    "graph_without_literals = rdflib.Graph()\n",
    "for s,p,o in movielens_dbpedia_2hops.triples((None, None, None)):\n",
    "  if o.startswith('http://dbpedia.org/resource/'):\n",
    "    graph_without_literals.add((s,p,o))\n",
    "\n",
    "graph_file = \"./movielens/2hopsDBpediaNoLiterals.nt\"\n",
    "graph_without_literals.serialize(destination=graph_file, format='nt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"movielens\"\n",
    "kg_type = \"Wikidata\"\n",
    "\n",
    "movielens_wikidata_2hops = expand_kg(kg_type, data, graph, num_hops = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"./movielens/2hopsWikidata.nt\"\n",
    "movielens_wikidata_2hops.serialize(destination=graph_file, format='nt')\n",
    "\n",
    "graph_without_literals = rdflib.Graph()\n",
    "for s,p,o in movielens_wikidata_2hops.triples((None, None, None)):\n",
    "  if o.startswith('http://www.wikidata.org/entity/Q'):\n",
    "    graph_without_literals.add((s,p,o))\n",
    "\n",
    "graph_file = \"./movielens/2hopsWikidataNoLiterals.nt\"\n",
    "graph_without_literals.serialize(destination=graph_file, format='nt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last.fm\n",
    "## DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T11:19:36.227626Z",
     "start_time": "2021-02-04T22:32:28.300859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8790 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________\n",
      "Running queue for...\n",
      "Data:\t\t          lastfm\n",
      "Knowledge Graph:\t          DBpedia\n",
      "Number of Hops:\t                2\n",
      "_____________________\n",
      "Parse all URls 1 hop(s) away...\n",
      "About to process 8790 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8790/8790 [00:53<00:00, 163.13it/s]\n",
      "  0%|          | 23/352082 [00:00<25:49, 227.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 hop(s).\n",
      "\n",
      "Parse all URls 2 hop(s) away...\n",
      "About to process 352082 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352082/352082 [12:46:13<00:00,  7.66it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2 hop(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = \"lastfm\"\n",
    "kg_type = \"DBpedia\"\n",
    "\n",
    "lastfm_dbpedia_2hops = expand_kg(kg_type, data, graph, num_hops = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T11:53:06.806808Z",
     "start_time": "2021-02-05T11:39:09.667612Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_file = \"./lastfm/2hopsDBpedia.nt\"\n",
    "lastfm_dbpedia_2hops.serialize(destination=graph_file, format='nt')\n",
    "\n",
    "graph_without_literals = rdflib.Graph()\n",
    "for s,p,o in lastfm_dbpedia_2hops.triples((None, None, None)):\n",
    "  if o.startswith('http://dbpedia.org/resource/'):\n",
    "    graph_without_literals.add((s,p,o))\n",
    "\n",
    "graph_file = \"./lastfm/2hopsDBpediaNoLiterals.nt\"\n",
    "graph_without_literals.serialize(destination=graph_file, format='nt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"lastfm\"\n",
    "kg_type = \"Wikidata\"\n",
    "\n",
    "lastfm_wikidata_2hops = expand_kg(kg_type, data, graph, num_hops = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"./lastfm/2hopsWikidata.nt\"\n",
    "lastfm_wikidata_2hops.serialize(destination=graph_file, format='nt')\n",
    "\n",
    "graph_without_literals = rdflib.Graph()\n",
    "for s,p,o in lastfm_wikidata_2hops.triples((None, None, None)):\n",
    "  if o.startswith('http://www.wikidata.org/entity/Q'):\n",
    "    graph_without_literals.add((s,p,o))\n",
    "\n",
    "graph_file = \"./lastfm/2hopsWikidataNoLiterals.nt\"\n",
    "graph_without_literals.serialize(destination=graph_file, format='nt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
