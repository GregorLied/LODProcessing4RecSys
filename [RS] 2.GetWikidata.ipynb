{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:40:30.696753Z",
     "start_time": "2021-02-04T20:40:30.037267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HDTStore at ...\n",
      "Number of RDF triples: 1137003322\n",
      "Number of subjects: 68940263\n",
      "Number of predicates: 123002\n",
      "Number of objects: 237607127\n",
      "Number of shared subject-object: 50711395\n"
     ]
    }
   ],
   "source": [
    "from rdflib_hdt import HDTStore\n",
    "from rdflib import Graph\n",
    "\n",
    "# Load an HDT file. Missing indexes are generated automatically\n",
    "# You can provide the index file by putting it in the same directory as the HDT file.\n",
    "# See https://www.rdfhdt.org/datasets/ for getting the HDT and the index file.\n",
    "print(\"Loading HDTStore at ...\")\n",
    "store = HDTStore(\"./dbpedia2016-10.hdt.1\")\n",
    "\n",
    "# Display some metadata about the HDT document itself\n",
    "print(f\"Number of RDF triples: {len(store)}\")\n",
    "print(f\"Number of subjects: {store.nb_subjects}\")\n",
    "print(f\"Number of predicates: {store.nb_predicates}\")\n",
    "print(f\"Number of objects: {store.nb_objects}\")\n",
    "print(f\"Number of shared subject-object: {store.nb_shared}\")\n",
    "\n",
    "# Create an RDFlib Graph with the HDT document as a backend\n",
    "graph = Graph(store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Wikidata URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:47:06.267102Z",
     "start_time": "2021-02-04T20:47:06.255045Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Namespace\n",
    "from rdflib.namespace import OWL\n",
    "from rdflib_hdt import HDTStore\n",
    "\n",
    "def get_wikdataURIs(data, graph):\n",
    "  \"\"\" Exploits owl:sameAs relation to get wikidata mappings\n",
    "  Args:\n",
    "      data (str): Specifies wether to get Wikidata mappings for movielens or lastfm\n",
    "      graph (Graph): RDFlib Graph with the HDT document as a backend.\n",
    "  \"\"\"\n",
    "\n",
    "  assert data in [\"movielens\", \"lastfm\"]\n",
    "\n",
    "  # Get for each item the corresponding DBPedia mappings/entities\n",
    "  df = pd.read_csv(f'./{data}/Mapping2DBpedia-1.2-corrected.tsv', sep='\\t', encoding = 'utf-8', header=None, engine='python')\n",
    "  entities = list(df[2])\n",
    "\n",
    "  # For each DBPedia mapping/entity:\n",
    "  # – Go through all the triples with owl:sameAs-relation where the mapping/entity is a subject\n",
    "  # – If there exists an object that starts with http://www.wikidata.org/entity/: \n",
    "  #   We found the corresponding Wikidata mapping for the item.\n",
    "  mapping_dict = {}\n",
    "  wikidata_not_found = []\n",
    "  for entity in tqdm(entities):\n",
    "    found_wikidata = False\n",
    "\n",
    "    # List of entities that had to be found manually.\n",
    "    if entity == \"http://dbpedia.org/resource/Big_Mike\":\n",
    "      mapping_dict[entity] = \"http://www.wikidata.org/entity/Q3609472\"\n",
    "      continue\n",
    "    \n",
    "    elif entity == \"http://dbpedia.org/resource/Dakota_(singer)\":\n",
    "      mapping_dict[entity] = \"http://www.wikidata.org/entity/Q27973731\"\n",
    "      continue\n",
    "\n",
    "    elif entity == \"http://dbpedia.org/resource/Avalon_(musician)\":\n",
    "      mapping_dict[entity] = \"http://www.wikidata.org/entity/Q28421675\"\n",
    "      continue   \n",
    "\n",
    "    for s, p, o in graph.triples((URIRef(entity), OWL.sameAs, None)):\n",
    "      if o.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "        mapping_dict[entity] = o\n",
    "        found_wikidata = True\n",
    "    \n",
    "    if not found_wikidata:\n",
    "      print(f\"No wikidata URI found for {entity}\")\n",
    "      wikidata_not_found.append(entity)\n",
    "\n",
    "  # Change URIs in df from DBPedia to Wikidata in order to save final mappings\n",
    "  for dbpediaURI, wikidataURI in mapping_dict.items():\n",
    "    index = df.loc[df[2] == dbpediaURI].index\n",
    "    df.at[index, 2] = wikidataURI\n",
    "\n",
    "  df.to_csv(f'./{data}/Mapping2Wikidata-1.2-corrected.tsv', sep='\\t', index = False, header = False, encoding=\"utf-8\")\n",
    "\n",
    "  return df, wikidata_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:47:41.286239Z",
     "start_time": "2021-02-04T20:47:21.180108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8790/8790 [00:01<00:00, 6904.80it/s]\n"
     ]
    }
   ],
   "source": [
    "#data = \"movielens\" \n",
    "data = \"lastfm\"\n",
    "\n",
    "df, wikidata_not_found = get_wikdataURIs(data, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T20:47:41.300766Z",
     "start_time": "2021-02-04T20:47:41.298657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
